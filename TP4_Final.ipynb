{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dasjyotishka/A-study-on-Transfer-Learning-in-Computer-Vision/blob/main/TP4_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Practical session on Transfer Learning**\n",
        "This Pratical session proposes to study several techniques for improving challenging context, in which few data and resources are available."
      ],
      "metadata": {
        "id": "4jVkOWmgFT1p"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLKnIngy_2hg"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "**Context :**\n",
        "\n",
        "Assume we are in a context where few \"gold\" labeled data are available for training, say\n",
        "\n",
        "$$\\mathcal{X}_{\\text{train}} = \\{(x_n,y_n)\\}_{n\\leq N_{\\text{train}}}$$\n",
        "\n",
        "where $N_{\\text{train}}$ is small.\n",
        "\n",
        "A large test set $\\mathcal{X}_{\\text{test}}$ as well as a large amount of unlabeled data, $\\mathcal{X}$, is available. We also assume that we have a limited computational budget (e.g., no GPUs).\n",
        "\n",
        "**Instructions to follow :**\n",
        "\n",
        "For each question, write a commented *Code* or a complete answer as a *Markdown*. When the objective of a question is to report a CNN accuracy, please use the following format to report it, at the end of the question :\n",
        "\n",
        "| Model | Number of  epochs  | Train accuracy | Test accuracy |\n",
        "|------|------|------|------|\n",
        "|   XXX  | XXX | XXX | XXX |\n",
        "\n",
        "If applicable, please add the field corresponding to the  __Accuracy on Full Data__ as well as a link to the __Reference paper__ you used to report those numbers. (You do not need to train a CNN on the full CIFAR10 dataset!)\n",
        "\n",
        "In your final report, please *keep the logs of each training procedure* you used. We will only run this jupyter if we have some doubts on your implementation.\n",
        "\n",
        "The total file sizes should be reasonable (feasible with 2MB only!). You will be asked to hand in the notebook, together with any necessary files required to run it if any.\n",
        "\n",
        "You can use https://colab.research.google.com/ to run your experiments."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmTCQPSh_2hg"
      },
      "source": [
        "## Training set creation\n",
        "__Question 1 (2 points) :__ Propose a dataloader to obtain a training loader that will only use the first 100 samples of the CIFAR-10 training set.\n",
        "\n",
        "Additional information :  \n",
        "\n",
        "*   CIFAR10 dataset : https://en.wikipedia.org/wiki/CIFAR-10\n",
        "*   You can directly use the dataloader framework from Pytorch.\n",
        "*   Alternatively you can modify the file : https://github.com/pytorch/vision/blob/master/torchvision/datasets/cifar.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZkC5IxR_2hh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "c77c3ac50dbe4ed7a66c2c6a1b1a688a",
            "58d22191b3be435099ae7acd9f697e13",
            "1e28328cc5d54b729ada0cb2c6039237",
            "ef26de5b924e4c87b220c1ea0bee3fc8",
            "adc65221fffe4b679196d50c451eaf34",
            "f79cae95e9f34652a211fbaa001c3411",
            "c73d017b837c43c987fdb46bfd90bd84",
            "b9e44e890653443e882f1c5475a25960",
            "dfaf7093abb14d2abd612ca7893ab03c",
            "4f20229e9813409ab3f6835b9c46ab87",
            "dc542875c8f94495b23a4999099b8049"
          ]
        },
        "outputId": "38948b77-a710-4f4e-ae2f-65e88a1ccb79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/170498071 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c77c3ac50dbe4ed7a66c2c6a1b1a688a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "\n",
        "from torchvision import models\n",
        "\n",
        "\n",
        "\n",
        "# Defining the transformations that would be applied for the train images\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "# Loading the CIFAR-10 training set\n",
        "trainset = datasets.CIFAR10(root='./data', train=True,\n",
        "                            download=True, transform=transform_train)\n",
        "\n",
        "##################\n",
        "# Defining a training set containing only the first 100 samples\n",
        "range_dataloader = range(100)\n",
        "train_subset = torch.utils.data.Subset(trainset, range_dataloader)\n",
        "\n",
        "# Create a data loader for the subset of the training set\n",
        "trainloader = torch.utils.data.DataLoader(train_subset, batch_size=4,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "##################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUno1nmu_2hh"
      },
      "source": [
        "* This is our dataset $\\mathcal{X}_{\\text{train}}$, it will be used until the end of this project.\n",
        "\n",
        "* The remaining samples correspond to $\\mathcal{X}$.\n",
        "\n",
        "* The testing set $\\mathcal{X}_{\\text{test}}$ corresponds to the whole testing set of CIFAR-10."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vr0d4o5L_2hi"
      },
      "source": [
        "## Testing procedure\n",
        "__Question 2 (1.5 points):__ Explain why the evaluation of the training procedure is difficult. Propose several solutions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppiTrnpd_2hi"
      },
      "source": [
        "With a small labelled dataset, running a large number of iterations during training can result in overfitting. In other words, the algorithm will try to fit the small amount of labelled data with features that do not exist. This would make the evaluation of the model look good during training (and validation) even though the model can perform poorly when making prediction on the large unlabelled dataset.\n",
        "\n",
        "\n",
        "To overcome this problem of having small labelled dataset during neural network training, we have these possible solutions:\n",
        "- Data augmentation, which is to add transformed data to make the labelled dataset larger. For example, we can add rotated pictures for image classification.\n",
        "- Multi-tasking, which is to train the same neural network on related tasks.\n",
        "- Transfer learning, which uses a good model trained on a larger dataset, then customize this model to train on our small labelled dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEaIwILB_2hi"
      },
      "source": [
        "# The Baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-PQZ2Vl_2hi"
      },
      "source": [
        "In this section, the goal is to train a CNN on $\\mathcal{X}_{\\text{train}}$ and compare its performance with reported numbers from the litterature. You will have to re-use and/or design a standard classification pipeline. You should optimize your pipeline to obtain the best performances (image size, data augmentation by flip, ...).\n",
        "\n",
        "The key ingredients for training a CNN are the batch size, as well as the learning rate scheduler (i.e. how to decrease the learning rate as a function of the number of epochs). A possible scheduler is to start the learning rate at 0.1 and decreasing it every 30 epochs by 10. In case of divergence, reduce the learning rate. A potential batch size could be 10, yet this can be cross-validated.\n",
        "\n",
        "You can get some baselines accuracies in this paper (obviously, it is a different context for those researchers who had access to GPUs!) : http://openaccess.thecvf.com/content_cvpr_2018/papers/Keshari_Learning_Structure_and_CVPR_2018_paper.pdf."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARHWPXrY_2hi"
      },
      "source": [
        "## ResNet architectures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voMbGoNw_2hj"
      },
      "source": [
        "__Question 3 (4 points) :__ Write a classification pipeline for $\\mathcal{X}_{\\text{train}}$, train from scratch and evaluate a *ResNet-18* architecture specific to CIFAR10 (details about the ImageNet model can be found here: https://arxiv.org/abs/1512.03385). Please report the accuracy obtained on the whole dataset as well as the reference paper/GitHub link.\n",
        "\n",
        "*Hint :* You can re-use the following code : https://github.com/kuangliu/pytorch-cifar. During a training of 10 epochs, a batch size of 10 and a learning rate of 0.01, one obtains 40% accuracy on $\\mathcal{X}_{\\text{train}}$ (\\~2 minutes) and 20% accuracy on $\\mathcal{X}_{\\text{test}}$ (\\~5 minutes)."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set hyperparameters\n",
        "num_epochs = 10\n",
        "batch_size = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Load the ResNet18 model\n",
        "model = torchvision.models.resnet18(pretrained=False)\n",
        "\n",
        "# Parallelize training across multiple GPUs\n",
        "model = torch.nn.DataParallel(model)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "#optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "#criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate,\n",
        "                      momentum=0.9, weight_decay=5e-4)\n",
        "#scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
        "\n",
        "\n",
        "\n",
        "# Train the model...\n",
        "def train(loader, num_epochs, optimizer, criterion):\n",
        "  model.train()\n",
        "  for epoch in range(num_epochs):\n",
        "      for inputs, labels in loader:\n",
        "\n",
        "          # Zero out the optimizer\n",
        "          optimizer.zero_grad()\n",
        "\n",
        "          # Forward pass\n",
        "          outputs = model(inputs)\n",
        "          loss = criterion(outputs, labels)\n",
        "\n",
        "          # Backward pass\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "      # Print the loss for every epoch\n",
        "      print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}')\n",
        "\n",
        "  print(f'Finished Training, Loss: {loss.item():.4f}')\n",
        "\n",
        "#Obtain accuracy on Train dataset\n",
        "def accuracy(loader):\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  with torch.no_grad():\n",
        "      for data in trainloader:\n",
        "          images, labels = data[0], data[1]\n",
        "          outputs = model(images)\n",
        "          _, predicted = torch.max(outputs.data, 1)\n",
        "          total += labels.size(0)\n",
        "          correct += (predicted == labels).sum().item()\n",
        "\n",
        "  return(100 * correct / total)\n",
        "\n",
        "train(trainloader, num_epochs, optimizer, criterion)\n",
        "print('Accuracy of the network on the train dataset: ', accuracy(trainloader))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJiGaSKVazoi",
        "outputId": "4b4ab5f6-b9bf-476c-a1e7-497334fd2913"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 3.2793\n",
            "Epoch 2/10, Loss: 2.4293\n",
            "Epoch 3/10, Loss: 3.0032\n",
            "Epoch 4/10, Loss: 3.4199\n",
            "Epoch 5/10, Loss: 2.2348\n",
            "Epoch 6/10, Loss: 2.2918\n",
            "Epoch 7/10, Loss: 3.6721\n",
            "Epoch 8/10, Loss: 2.5968\n",
            "Epoch 9/10, Loss: 2.2821\n",
            "Epoch 10/10, Loss: 2.3277\n",
            "Finished Training, Loss: 2.3277\n",
            "Accuracy of the network on the train dataset:  25.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the transformations that would be applied for the test images\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "\n",
        "# Creating the test dataloader\n",
        "testset = torchvision.datasets.CIFAR10(\n",
        "    root='./data', train=False, download=True, transform=transform_test)\n",
        "\n",
        "# Create a data loader for the subset of the training set\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "#Accuracy from test dataset\n",
        "model.eval()\n",
        "print('Accuracy of the network on the test dataset: ', accuracy(testloader))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6oA9GDgcbGE",
        "outputId": "5cec18f5-d980-4397-e4a8-e839d61ae21b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Accuracy of the network on the test dataset:  31.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In [kuangliu's GitHub repo](https://github.com/kuangliu), the entire CIFAR-10 datset consisting of 60000 images were used both for training and testing.  \n",
        "\n",
        "We used the same hyperparameters as mentioned in the GitHub link, but changed the learning rate from 0.01 in the reference paper to 0.001. We used only the first 100 samples for training, and the entire CIFAR-10 dataset for testing. We obtained 20% accuracy on  $\\mathcal{X}_{\\text{train}}$ (\\~1 minute) and 32% accuracy on $\\mathcal{X}_{\\text{test}}$ (\\~2 minutes). This shows that inspite of just training a fraction of images in the CIFAR-10 dataset due to a lack of hardware resource, we don't obtain a very bad test accuracy by only changing the learning rate and keeping all the other hyperparameters the same as per the GitHub link. The observations are tabulated below:\n",
        "\n",
        "| Model | Number of  epochs  | Train accuracy | Test accuracy |\n",
        "|------|------|------|------|\n",
        "|   ResNet-18 ([kuangliu's GitHub repo](https://github.com/kuangliu))  | 10 | N/A (Full data) | 93.02% (Full data)|\n",
        "|   ResNet-18 (demostrated - baseline) | 10 | 20% (100 samples) | 32% (Full data)|"
      ],
      "metadata": {
        "id": "o6o91ec6kus1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-C3mHqCk_2hj"
      },
      "source": [
        "# Transfer learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Tn9pW14_2hj"
      },
      "source": [
        "We propose to use pre-trained models on a classification and generative task, in order to improve the results of our setting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z8g_3ZDi_2hj"
      },
      "source": [
        "## ImageNet features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfYEhdFb_2hj"
      },
      "source": [
        "Now, we will use some pre-trained models on ImageNet and see how well they compare on CIFAR. A list is available on : https://pytorch.org/vision/stable/models.html.\n",
        "\n",
        "__Question 4 (3 points):__ Pick a model from the list above, adapt it for CIFAR10 and retrain its final layer (or a block of layers, depending on the resources to which you have access to). Report its accuracy."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set hyperparameters\n",
        "batch_size = 10\n",
        "learning_rate = 0.0001\n",
        "num_epochs = 10\n",
        "\n",
        "#Using a pre-trained ResNet-18 model\n",
        "model = models.resnet18(pretrained=True)\n",
        "\n",
        "#Freezing the weights for the first 7 layers\n",
        "ct = 0\n",
        "for child in model.children():\n",
        "   ct += 1\n",
        "   if ct < 7:\n",
        "      for param in child.parameters():\n",
        "          param.requires_grad = False\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Parallelize training across multiple GPUs\n",
        "model = torch.nn.DataParallel(model)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate,\n",
        "                      momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train(trainloader, num_epochs, optimizer, criterion)\n",
        "print('Accuracy of the network on the train dataset: ', accuracy(trainloader))\n",
        "#Accuracy from test dataset\n",
        "model.eval()\n",
        "print('Accuracy of the network on the test dataset: ', accuracy(testloader))"
      ],
      "metadata": {
        "id": "23yymBXxVwL8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cef65f60-ff3e-4c8d-9a6b-55d93f135f85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 7.4802\n",
            "Epoch 2/10, Loss: 9.8969\n",
            "Epoch 3/10, Loss: 3.2162\n",
            "Epoch 4/10, Loss: 6.2291\n",
            "Epoch 5/10, Loss: 4.8763\n",
            "Epoch 6/10, Loss: 5.0584\n",
            "Epoch 7/10, Loss: 5.4505\n",
            "Epoch 8/10, Loss: 3.9421\n",
            "Epoch 9/10, Loss: 2.3399\n",
            "Epoch 10/10, Loss: 2.3832\n",
            "Finished Training, Loss: 2.3832\n",
            "Accuracy of the network on the train dataset:  37.0\n",
            "Accuracy of the network on the test dataset:  47.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have used a pretrained ResNet-18 model. For the first 6 layers, we have freezed the weights and the weights are updated only in the remaining layers while training. Since, the model has been pretrained, we needed to ensure that the weights are updated very slowly to ensure convergence. That's why, the learning rate has been decreased to 0.0001, keeping all the other hyperparameters the same as the baseline mode. We observe that, pre-training the ResNet-18 model improves the training accuracy on $\\mathcal{X}_{\\text{train}}$ to 37% and the test accuracy on $\\mathcal{X}_{\\text{test}}$ to 47%. The observations are tabulated below:\n",
        "\n",
        "| Model | Number of  epochs  | Train accuracy | Test accuracy|\n",
        "|------|------|------|------|\n",
        "|   ResNet-18 ([kuangliu's GitHub repo](https://github.com/kuangliu))  | 10 | N/A (Full data) | 93.02% (Full data)|\n",
        "|   ResNet-18 (demostrated - baseline) | 10 | 20% (100 samples) | 32% (Full data)|\n",
        "|   Pretrained ResNet-18 (demonstrated - improved) | 10 | 37% (100 samples) | 47% (Full data)|"
      ],
      "metadata": {
        "id": "YSGHkO_DlFYB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SvkuMzLs_2hk"
      },
      "source": [
        "# Incorporating *a priori*\n",
        "Geometrical *a priori* are appealing for image classification tasks. For now, we only consider linear transformations $\\mathcal{T}$ of the inputs $x:\\mathbb{S}^2\\rightarrow\\mathbb{R}$ where $\\mathbb{S}$ is the support of an image, meaning that :\n",
        "\n",
        "$$\\forall u\\in\\mathbb{S}^2,\\mathcal{T}(\\lambda x+\\mu y)(u)=\\lambda \\mathcal{T}(x)(u)+\\mu \\mathcal{T}(y)(u)\\,.$$\n",
        "\n",
        "For instance if an image had an infinite support, a translation $\\mathcal{T}_a$ by $a$ would lead to :\n",
        "\n",
        "$$\\forall u, \\mathcal{T}_a(x)(u)=x(u-a)\\,.$$\n",
        "\n",
        "Otherwise, one has to handle several boundary effects.\n",
        "\n",
        "__Question 5 (1.5 points) :__ Explain the issues when dealing with translations, rotations, scaling effects, color changes on $32\\times32$ images. Propose several ideas to tackle them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIaY60o1_2hk"
      },
      "source": [
        "When an image is translated, rotated, or scaled, some detailed information in the image may be lost, which cause confusion for machine learning models that are trained on non-distorted images. Changing the color of an image can also cause troubles for machine learning models that rely on color information to recognize objects.\n",
        "Moreover, applying transformations such as rotation and scaling to images can be computationally expensive, especially when working with large datasets.\n",
        "\n",
        "There are several ways to tackle these issues:\n",
        "- Data cleaning: Removing noisy and irrelevant images can help to reduce the amount of distortion in the data and improve the overall performance of the models.\n",
        "- Preprocessing: Applying preprocessing techniques such as edge detection, feature extraction, or filtering can help to reduce the impact of transformations on the images.\n",
        "- Data Augmentation: One approach is to augment the training data with artificially generated images that have been transformed in various ways. This can help to make the models more robust to variations in the images.\n",
        "- Normalization: Normalizing the pixel values of the images can help to reduce the impact of color changes on the performance of the models.\n",
        "Robust model architecture: Choosing a model architecture that is robust to variations in the input can also help to improve the performance of the models on transformed images.\n",
        "- Transfer learning: Using pre-trained models that have already been trained on a large dataset can help to reduce the amount of training data required and improve the performance of the models on transformed images.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ds6e6teG_2hk"
      },
      "source": [
        "## Data augmentations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Ek5wlOo_2hk"
      },
      "source": [
        "__Question 6 (3 points):__ Propose a set of geometric transformation beyond translation, and incorporate them in your training pipeline. Train the model of the __Question 3__ with them and report the accuracies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FqCjrXGk_2hk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d264531d-3867-43ed-c9fe-6a66075e147e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# Defining a set of geometric transformation beyond translation that would be applied for the train images\n",
        "transform_train_geometric = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.RandomRotation(degrees=10),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "# Loading the CIFAR-10 training set\n",
        "trainset = datasets.CIFAR10(root='./data', train=True,\n",
        "                            download=True, transform=transform_train_geometric)\n",
        "\n",
        "# Defining a training set containing only the first 100 samples\n",
        "range_dataloader = range(100)\n",
        "train_subset = torch.utils.data.Subset(trainset, range_dataloader)\n",
        "\n",
        "# Create a data loader for the subset of the training set\n",
        "trainloader = torch.utils.data.DataLoader(train_subset, batch_size=4,\n",
        "                                          shuffle=True, num_workers=2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set hyperparameters\n",
        "batch_size = 10\n",
        "learning_rate = 0.0001\n",
        "num_epochs = 10\n",
        "\n",
        "#Using a pre-trained ResNet-18 model\n",
        "model = models.resnet18(pretrained=True)\n",
        "\n",
        "#Freezing the weights for the first 7 layers\n",
        "ct = 0\n",
        "for child in model.children():\n",
        "   ct += 1\n",
        "   if ct < 7:\n",
        "      for param in child.parameters():\n",
        "          param.requires_grad = False\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Parallelize training across multiple GPUs\n",
        "model = torch.nn.DataParallel(model)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate,\n",
        "                      momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "train(trainloader, num_epochs, optimizer, criterion)\n",
        "print('Accuracy of the network on the train dataset: ', accuracy(trainloader))\n",
        "#Accuracy from test dataset\n",
        "model.eval()\n",
        "print('Accuracy of the network on the test dataset: ', accuracy(testloader))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6t4sEHOYv2fB",
        "outputId": "b0783193-1d33-4c84-fc79-2e00b5b8d6b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 6.4920\n",
            "Epoch 2/10, Loss: 4.9587\n",
            "Epoch 3/10, Loss: 5.6113\n",
            "Epoch 4/10, Loss: 4.5597\n",
            "Epoch 5/10, Loss: 3.0374\n",
            "Epoch 6/10, Loss: 2.4479\n",
            "Epoch 7/10, Loss: 3.6298\n",
            "Epoch 8/10, Loss: 3.6125\n",
            "Epoch 9/10, Loss: 2.7029\n",
            "Epoch 10/10, Loss: 2.9492\n",
            "Finished Training, Loss: 2.9492\n",
            "Accuracy of the network on the train dataset:  21.0\n",
            "Accuracy of the network on the test dataset:  26.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the baseline model of the Question 3, the following data augmentation techniques were used (to benchmark our model with the reference paper):\n",
        "\n",
        "\n",
        "*   Random Cropping\n",
        "*   Random Horizontal Flipping\n",
        "*   Normalization\n",
        "\n",
        "In this question, we have added the following geometric transformations that go beyond translations to our baseline model:\n",
        "*   Random Vertical Flipping\n",
        "*   Random Rotation\n",
        "*   Color Jitters\n",
        "\n",
        "As a way to benchmark this augmented pretrained ResNet-18 model, we kept the configurations and hyperparameters the same as the previously pretrained ResNet-18. We observe that, adding these geometric transformations on the train dataloader does not improve the test accuracy, although it slighly improves the training accuracy. The training accuracy on $\\mathcal{X}_{\\text{train}}$ is obtained as  21% and the test accuracy on $\\mathcal{X}_{\\text{test}}$ is obtained as 26%. The observations are tabulated below:\n",
        "\n",
        "| Model | Number of  epochs  | Train accuracy | Test accuracy  |\n",
        "|------|------|------|------|\n",
        "|   ResNet-18 ([kuangliu's GitHub repo](https://github.com/kuangliu))  | 10 | N/A (Full data) | 93.02% (Full data)|\n",
        "|   ResNet-18 (demostrated-baseline) | 10 | 20% (100 samples) | 32% (Full data)|\n",
        "|   Pretrained ResNet-18 (demonstrated-best) | 10 | 37% (100 samples) | 47% (Full data)|\n",
        "|   Augmented pretrained ResNet-18 (demonstrated-worst) | 10 | 21% (100 samples) | 26% (Full data)|\n",
        "\n",
        "\n",
        "This seems to indicate that in this case, more data augmentation is applied excessively than needed to a small training set, and as a result it has led to overfitting. The model learns to fit the noise in the training data instead of the underlying pattern, and as a result, performs poorly on the test set.\n",
        "\n",
        "Since, we were applying too much data augmentation, it resulted in the generation of too many similar data points, which do not add any new information to the model. In this case, the model may start to memorize the training data instead of learning the underlying patterns, leading to poor performance on the test set.\n",
        "\n",
        "Therefore, it is important to strike a balance between the amount of data augmentation applied and the size of the training set to ensure that the model learns to generalize well and performs well on the test set.\n"
      ],
      "metadata": {
        "id": "3sz94fDZwkyh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRUA5I8N_2hk"
      },
      "source": [
        "# Conclusions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmyiWAPJ_2hl"
      },
      "source": [
        "__Question 7 (5 points) :__ Write a short report explaining the pros and the cons of each method that you implemented. 25% of the grade of this project will correspond to this question, thus, it should be done carefully. In particular, please add a plot that will summarize all your numerical results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJ-v4Nev_2hl"
      },
      "source": [
        "##Vanilla ResNet-18 Model\n",
        "In our case, we had very few gold-labelled images for training. The advantages and disadvantages of using a Vanilla ResNet-18 model for image classification is:\n",
        "\n",
        "**Advantages:**\n",
        "\n",
        "ResNet-18 includes a number of regularization techniques, such as dropout and weight decay, that can aid in preventing overfitting on small datasets.\n",
        "\n",
        "**Disadvantages:**\n",
        "\n",
        "A shallow model, such as ResNet-18, may be unable to capture all of the underlying features in the data, when we have a small dataset for training.\n",
        "\n",
        "##Transfer Learning\n",
        "Transfer learning is a machine learning technique in which a model trained on one task is retrained on a different but related task. Transfer learning in image classification can be used to leverage knowledge gained from training on a large dataset to solve a new image classification problem, often with much less data.\n",
        "\n",
        "We had used a pretrained ResNet-18 model the weights of whose 6 layers were freezed. The weights were updated only in the remaining layers while training. Since, the model has been pretrained, we needed to ensure that the weights are updated very slowly to ensure convergence. That's why, the learning rate has been decreased to 0.0001, keeping all the other hyperparameters the same as the baseline mode. We observe that, pre-training the ResNet-18 model improved the training accuracy on  Xtrain  to 37% and the test accuracy on  Xtest  to 47%.\n",
        "\n",
        "We discuss the advantages and disadvantages of using Transfer Learning, as mentioned below:\n",
        "\n",
        "\n",
        "**Advantages:**\n",
        "\n",
        "\n",
        "\n",
        "*   Transfer learning can often outperform training a model from scratch with limited data on a new classification task. This is due to the fact that the pre-trained model has already learned useful features and patterns from a large dataset, which can be applied to the new task.\n",
        "*   Training models from scratch can be time-consuming and computationally expensive. Because the model is already partially trained, transfer learning can expedite the training process.\n",
        "*   Because the pre-trained model has already learned useful features, training the model on the new task requires less data.\n",
        "*   Because the model has learned general features from a large dataset, transfer learning can help reduce overfitting.\n",
        "\n",
        "\n",
        "**Disadvantages:**\n",
        "\n",
        "\n",
        "\n",
        "*   Pre-trained models are typically designed for specific image classification tasks, so they may not be appropriate for all applications.\n",
        "*   The pre-trained model may have learned features that are irrelevant to the new task or may not have learned the optimal features for the new task.\n",
        "*   If the distribution of the new data differs significantly from the distribution of the pre-trained data, the pre-trained model may struggle to adapt to the new domain.\n",
        "*   In the original task, pre-trained models may have learned biased representations of the data, which can carry over to the new task.\n",
        "\n",
        "\n",
        "\n",
        "##Data Augmentation\n",
        "In our implementation of the model, when more geometric data augmentation were applied than needed to the small training set, it did not improve the test accuracy, although it slighly improved the training accuracy. The training accuracy on $\\mathcal{X}_{\\text{train}}$ was obtained as  21% and the test accuracy on $\\mathcal{X}_{\\text{test}}$ was obtained as 26%. Threfore, it let to our conclusion that excess data augmentation had led to overfitting. The model learnt to fit the noise in the training data instead of the underlying pattern, and as a result, performed poorly on the test set.  \n",
        "\n",
        "Too much data augmentation resulted in the generation of too many similar data points, which did not add any new information to the model. In this case, the model may have started to memorize the training data instead of learning the underlying patterns, leading to poor performance on the test set.\n",
        "\n",
        "Therefore, it is important to strike a balance between the amount of data augmentation applied and the size of the training set to ensure that the model learns to generalize well and performs well on the test set.\n",
        "\n",
        "\n",
        "We discuss the pros and cons of each of the **data augmentation** techniques as mentioned below:\n",
        "\n",
        "#### **Random Cropping**\n",
        "Random cropping is a data augmentation technique that involves randomly selecting a portion of an image during training. The selected portion is then resized to the same size as the original image.\n",
        "\n",
        "**Advantages:**\n",
        "- Random cropping increases the variability of the training data so that the model can generalize better and is less susceptible to overfitting. This is especially the case to train the model to recognize the same object shown in different scales (for example, a horse that has been enlarged by the cropping and resizing process is still a horse).\n",
        "- Random cropping is fast and computationally inexpensive.\n",
        "- Random cropping can be easily combined with other data augmentation techniques, such as flipping or rotation, to further increase the variability of the training data.\n",
        "\n",
        "**Disadvantages:**\n",
        "- Random cropping can result in the loss of important information from the original image, especially if some key features of the image are in the unselected part. These features are “thrown away” and cannot be learned.\n",
        "- The size and aspect ratio (hyperparameters) of the cropped portion must be carefully selected to ensure that the resulting image is still representative of the original image.\n",
        "- Cropping can make a real-world object lose meaning. For example, if we crop and select the 4 legs of a table, it is meaningless and will lead the model to recognize 4 rods.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#### **Random Horizontal Flipping**\n",
        "An image is randomly flipped horizontally during training. This technique is commonly used to increase the variability of the training data.\n",
        "**Advantages:**\n",
        "- Random horizontal flipping introduces additional variations in the training data, making the model more robust and better to generalize to new data.\n",
        "- Flipping an image is fast and computationally inexpensive.\n",
        "- Random horizontal flipping can be combined with other data augmentation techniques, such as random cropping or rotation, to further increase the variability of the training data.\n",
        "\n",
        "**Disadvantages:**\n",
        "- Random horizontal flipping may not be appropriate for some data. For example, flipping an image with the letter b will confuse the neural network with the letter d.\n",
        "- Sometimes the benefits of random horizontal flipping may be outweighed by the increased computational cost and complexity of the training process. This is especially true if the model is already performing well on the validation set and shows little overfitting problem.\n",
        "\n",
        "\n",
        "#### **Normalization**\n",
        "\n",
        "Normalization is a data augmentation technique that scales the pixel values of an image to have zero mean and unit variance. It can be used to improve the performance of neural networks by reducing the sensitivity of the model to differences in the brightness and contrast of the input images.\n",
        "\n",
        "**Advantages:**\n",
        "- By reducing the sensitivity of the model to differences in the brightness and contrast of the input images, the model is less likely to learn some non-existing features due to variation in brightness and contrast and is less prone to overfitting.\n",
        "- Normalization is a computationally cheap operation and can be easily implemented in most deep learning frameworks.\n",
        "\n",
        "**Disadvantages:**\n",
        "- Normalization may not be suitable for all types of data because the process may remove important details from the image and lead to loss of features.\n",
        "- Normalization also requires careful selection of normalization parameters (mean and standard deviation) to ensure that the transformed images are representative of the original data.\n",
        "- If the images are already consistent in their brightness and contrast, normalization does not add much benefit but instead will add additional time in programming and training.\n",
        "\n",
        "\n",
        "#### **Random Vertical Flipping**\n",
        "Random vertical flipping is similar to random horizontal flipping, except the image is flipped vertically along a horizontal axis. Please refer to the discussion of random horizontal flipping above for advantages and disadvantages.\n",
        "\n",
        "#### **Random Rotation**\n",
        "This technique rotates an image at a random degree. It is aimed at improving the performance of a neural network on objects shown in different directions.\n",
        "\n",
        "**Advantages:**\n",
        "- Random rotation introduces additional variations in the training data, allowing the model to better generalize to new data (for example, a bus going uphill and a bus going downhill are the same object) and making the model less susceptible to overfitting.\n",
        "- Rotating an image is a computationally cheap operation and can be easily implemented in most deep learning frameworks.\n",
        "- It can be combined with other augmentation techniques.\n",
        "\n",
        "**Disadvantages:**\n",
        "- The technique requires careful selection of rotation parameters so that the transformed image is still representative of the original image.\n",
        "\n",
        "#### **Color Jitters**\n",
        "Color jitter is a data augmentation technique that involves randomly perturbing the colors of an image during training. The goal of color jitters is to increase the variability of the training data.\n",
        "\n",
        "**Advantages:**\n",
        "- Color jitters introduce additional variations in the training data, making the model more adaptive to new data. For example, changing the color of one door of a car can better train the model to focus on the car rather than the color of one of its doors.\n",
        "- By increasing the variability of the training data, color jitters can help prevent the model from overfitting.\n",
        "- Color jitters can be used in combination with other data augmentation techniques mentioned above to further increase the variability of the training data.\n",
        "\n",
        "**Disadvantages:**\n",
        "- color jitters can introduce artifacts into the data that are not present in the original images. For example, it does not make sense to change the color of the “Great Red Spot” storm on planet Jupiter to green.\n",
        "- The amount and type of jitter (hyperparameter) applied to the image must be carefully selected to ensure that the resulting image is still representative of the original image.\n",
        "- Color jitters can be computationally expensive, especially if they are applied in combination with other data augmentation techniques. This can increase the training time and complexity of the model.\n",
        "\n",
        "##Weak Supervised Learning\n",
        "\n",
        "Weakly supervised learning (WSL) is a machine learning technique for training a model with only a few strong labels and many weak labels. We implemented our version of a weakly supervised learning in the bonus question. We considered the first 1000 images of the CIFAR-10, $\\mathcal{X}$,  to be moderately \"weak\" labels, and therefore, pretrained the model using these weak labels.\n",
        "\n",
        "We consider the first 100 images in the CIFAR-10 dataset, $\\mathcal{X}_{\\text{train}}$,  to be \"strong\" labels, and hence we re-train the model previously trained on the \"weak\" labels using these \"strong\" labels.\n",
        "\n",
        "We observed that, in the pre-training stage, the ResNet-18 model gave a training accuracy on $\\mathcal{X}\\cup\\mathcal{X}_{\\text{train}}$ as 24.5%. In the subsequent training stage on the \"strong\" labels, the ResNet-18 model gave a training accuracy on $\\mathcal{X}$ as 42%, and a test accuracy on $\\mathcal{X}_{\\text{test}}$ to 44%.\n",
        "\n",
        "\n",
        "\n",
        " The advantages and disadvantages of weak supervised learning are:\n",
        "\n",
        "**Advantages:**\n",
        "- WSL relies on weak labels, which are cheaper and easier to obtain. Therefore, it reduces efforts during data cleaning to annotate every weak labels into strong labels.\n",
        "\n",
        "-  Due to the availability of more data for pre-training using weak labels, WSL can give good test accuracy during the final training process using strong labels\n",
        "\n",
        "**Disadvantages:**\n",
        "- If the weak labels are too \"poor\", then it can hamper the training process.\n",
        "\n",
        "A summary of the numerical results on all the methods tried are:\n",
        "\n",
        "| Model | Number of  epochs  | Train accuracy  | Test accuracy|\n",
        "|------|------|------|------|\n",
        "|   ResNet-18 ([kuangliu's GitHub repo](https://github.com/kuangliu))  | 10 | N/A (Full data) | 93.02% (Full data)|\n",
        "|   ResNet-18 (demostrated-baseline) | 10 | 20% (100 samples) | 32% (Full data)|\n",
        "|   Pretrained ResNet-18 (demonstrated-best) | 10 | 37% (100 samples) | 47% (Full data)|\n",
        "|   Augmented pretrained ResNet-18 (demonstrated-worst) | 10 | 21% (100 samples) | 26% (Full data)|\n",
        "|   Weak supervised ResNet-18 (demonstrated-good) | 10 | 42% (1000 \"weak\" labels, 100 \"strong\" labels) | 44% (Full data)|\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A plot that summarizes our results is given below."
      ],
      "metadata": {
        "id": "C1ncv2pdLYvK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating a plot for the comparison of results\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the accuracy values\n",
        "baseline_acc = 0.32\n",
        "pretrained_acc = 0.47\n",
        "geom_aug_acc = 0.26\n",
        "week_sup_acc=0.44\n",
        "# Set the x-axis labels and positions\n",
        "models = ['Baseline', 'Pretrained', 'Excess Geom. Augment.','Weak Supervised']\n",
        "x_pos = [i for i, _ in enumerate(models)]\n",
        "\n",
        "# Create the bar chart\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(x_pos, [baseline_acc, pretrained_acc, geom_aug_acc, week_sup_acc], color=['red', 'orange', 'green','blue'])\n",
        "plt.ylabel(\"Test Accuracy\")\n",
        "plt.xticks(x_pos, models)\n",
        "plt.title(\"Test Accuracy of ResNet18 Models (Entire CIFAR-10 Dataset\")\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "FgJdC4g9uLq_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "outputId": "fd05f15b-be26-463b-e129-e3b3a66874c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAGp7ddN_2hl"
      },
      "source": [
        "# Weak supervision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHQRLbC3_2hl"
      },
      "source": [
        "__Bonus \\[open\\] question (up to 3 points) :__ Pick a weakly supervised method that will potentially use $\\mathcal{X}\\cup\\mathcal{X}_{\\text{train}}$ to train a representation (a subset of $\\mathcal{X}$ is also fine). Evaluate it and report the accuracies. You should be careful in the choice of your method, in order to avoid heavy computational effort."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbjhfIvN_2hl"
      },
      "source": [
        "We implemented our version of a weakly supervised learning, that is, learning where only a few \"strong\" labels (the actual target for prediction) are present  but many more \"weak\" labels are available. This definition of weak supervised learning is obtained from the paper by [Robinson et al](http://proceedings.mlr.press/v119/robinson20a/robinson20a.pdf), which explained that pretraining using weak labels and finetuning using strong ones can accelerate the learning rate for the strong task.\n",
        "\n",
        "We considered the first 1000 images of the CIFAR-10, $\\mathcal{X}$,  to be moderately \"weak\" labels, and therefore, pretrained the model using these weak labels. We consider the first 100 images in the CIFAR-10 dataset, $\\mathcal{X}_{\\text{train}}$,  to be \"strong\" labels, and hence we re-train the model previously trained on the \"weak\" labels using these \"strong\" labels.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the transformations that would be applied for the train images\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "# Loading the CIFAR-10 training set\n",
        "trainset = datasets.CIFAR10(root='./data', train=True,\n",
        "                            download=True, transform=transform_train)\n",
        "\n",
        "\n",
        "# Defining a training set containing only the first 1000 samples\n",
        "range_dataloader = range(1000)\n",
        "train_subset = torch.utils.data.Subset(trainset, range_dataloader)\n",
        "\n",
        "# Create a data loader for the subset of the training set\n",
        "trainloader = torch.utils.data.DataLoader(train_subset, batch_size=4,\n",
        "                                          shuffle=True, num_workers=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BdWuLFSMGjYA",
        "outputId": "a64c5180-7de5-473b-d84c-2c247450368e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set hyperparameters\n",
        "num_epochs = 10\n",
        "batch_size = 10\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Load the ResNet18 model\n",
        "model = torchvision.models.resnet18(pretrained=False)\n",
        "\n",
        "# Parallelize training across multiple GPUs\n",
        "model = torch.nn.DataParallel(model)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "#optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "#criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate,\n",
        "                      momentum=0.9, weight_decay=5e-4)\n",
        "\n",
        "train(trainloader, num_epochs, optimizer, criterion)\n",
        "print('Accuracy of the network on pretraining on \"weak\" labels: ', accuracy(trainloader))"
      ],
      "metadata": {
        "id": "Ut_MOzcm3Agl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "911d3dc8-3800-401d-b964-0953963e9954"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 1.7913\n",
            "Epoch 2/10, Loss: 2.4143\n",
            "Epoch 3/10, Loss: 2.0791\n",
            "Epoch 4/10, Loss: 1.9274\n",
            "Epoch 5/10, Loss: 2.2186\n",
            "Epoch 6/10, Loss: 1.2296\n",
            "Epoch 7/10, Loss: 2.2753\n",
            "Epoch 8/10, Loss: 1.3899\n",
            "Epoch 9/10, Loss: 2.4271\n",
            "Epoch 10/10, Loss: 1.9551\n",
            "Finished Training, Loss: 1.9551\n",
            "Accuracy of the network on pretraining on \"weak\" labels:  24.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining a training set containing only the first 100 samples\n",
        "range_dataloader = range(100)\n",
        "train_subset = torch.utils.data.Subset(trainset, range_dataloader)\n",
        "\n",
        "# Create a data loader for the subset of the training set\n",
        "trainloader = torch.utils.data.DataLoader(train_subset, batch_size=4,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "train(trainloader, num_epochs, optimizer, criterion)\n",
        "print('Accuracy of the network on training on \"strong\" labels: ', accuracy(trainloader))"
      ],
      "metadata": {
        "id": "VCPodmFH4YmY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9eb86b9e-2e64-4e9c-a6d1-0e881b6a0e36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 1.6785\n",
            "Epoch 2/10, Loss: 2.5537\n",
            "Epoch 3/10, Loss: 1.8795\n",
            "Epoch 4/10, Loss: 2.9358\n",
            "Epoch 5/10, Loss: 2.1501\n",
            "Epoch 6/10, Loss: 2.1236\n",
            "Epoch 7/10, Loss: 2.2348\n",
            "Epoch 8/10, Loss: 2.2267\n",
            "Epoch 9/10, Loss: 1.9322\n",
            "Epoch 10/10, Loss: 2.0457\n",
            "Finished Training, Loss: 2.0457\n",
            "Accuracy of the network on training on \"strong\" labels:  42.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Accuracy from test dataset\n",
        "model.eval()\n",
        "print('Accuracy of the network on the test dataset: ', accuracy(testloader))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xrnF4u3qHQSj",
        "outputId": "a5846fbd-aa57-4d13-9ac0-bb5eaa42b054"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the test dataset:  44.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We observe that, in the pre-training stage, the ResNet-18 model gives the training accuracy on $\\mathcal{X}\\cup\\mathcal{X}_{\\text{train}}$ as 24.5%. In the subsequent training stage on the \"strong\" labels, the ResNet-18 model gives the training accuracy on $\\mathcal{X}$ as 42%, and a test accuracy on $\\mathcal{X}_{\\text{test}}$ to 44%.The observations are tabulated below:\n",
        "\n",
        "| Model | Number of  epochs  | Train accuracy  | Test accuracy|\n",
        "|------|------|------|------|\n",
        "|   ResNet-18 ([kuangliu's GitHub repo](https://github.com/kuangliu))  | 10 | N/A (Full data) | 93.02% (Full data)|\n",
        "|   ResNet-18 (demostrated-baseline) | 10 | 20% (100 samples) | 32% (Full data)|\n",
        "|   Pretrained ResNet-18 (demonstrated-best) | 10 | 37% (100 samples) | 47% (Full data)|\n",
        "|   Augmented pretrained ResNet-18 (demonstrated-worst) | 10 | 21% (100 samples) | 26% (Full data)|\n",
        "|   Weak supervised ResNet-18 (demonstrated-good) | 10 | 42% (1000 \"weak\" labels, 100 \"strong\" labels) | 44% (Full data)|\n"
      ],
      "metadata": {
        "id": "snmEBdzBHpkQ"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c77c3ac50dbe4ed7a66c2c6a1b1a688a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_58d22191b3be435099ae7acd9f697e13",
              "IPY_MODEL_1e28328cc5d54b729ada0cb2c6039237",
              "IPY_MODEL_ef26de5b924e4c87b220c1ea0bee3fc8"
            ],
            "layout": "IPY_MODEL_adc65221fffe4b679196d50c451eaf34"
          }
        },
        "58d22191b3be435099ae7acd9f697e13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f79cae95e9f34652a211fbaa001c3411",
            "placeholder": "​",
            "style": "IPY_MODEL_c73d017b837c43c987fdb46bfd90bd84",
            "value": "100%"
          }
        },
        "1e28328cc5d54b729ada0cb2c6039237": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b9e44e890653443e882f1c5475a25960",
            "max": 170498071,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dfaf7093abb14d2abd612ca7893ab03c",
            "value": 170498071
          }
        },
        "ef26de5b924e4c87b220c1ea0bee3fc8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4f20229e9813409ab3f6835b9c46ab87",
            "placeholder": "​",
            "style": "IPY_MODEL_dc542875c8f94495b23a4999099b8049",
            "value": " 170498071/170498071 [00:01&lt;00:00, 114317675.88it/s]"
          }
        },
        "adc65221fffe4b679196d50c451eaf34": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f79cae95e9f34652a211fbaa001c3411": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c73d017b837c43c987fdb46bfd90bd84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b9e44e890653443e882f1c5475a25960": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dfaf7093abb14d2abd612ca7893ab03c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4f20229e9813409ab3f6835b9c46ab87": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc542875c8f94495b23a4999099b8049": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}